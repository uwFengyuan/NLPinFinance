{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "from random import shuffle, seed\n",
    "import re\n",
    "\n",
    "seed(123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cell_Phones_and_Accessories.pkl',\n",
       " 'Office_Products.pkl',\n",
       " 'Movies_and_TV.pkl',\n",
       " 'Patio_Lawn_and_Garden.pkl',\n",
       " 'Clothing_Shoes_and_Jewelry.pkl',\n",
       " 'Electronics.pkl',\n",
       " 'Industrial_and_Scientific.pkl',\n",
       " 'Kindle_Store.pkl',\n",
       " 'Digital_Music.pkl',\n",
       " 'Home_and_Kitchen.pkl',\n",
       " 'Sports_and_Outdoors.pkl',\n",
       " 'CDs_and_Vinyl.pkl',\n",
       " 'Books.pkl',\n",
       " 'Arts_Crafts_and_Sewing.pkl',\n",
       " 'Toys_and_Games.pkl',\n",
       " 'Grocery_and_Gourmet_Food.pkl',\n",
       " 'AMAZON_FASHION.pkl',\n",
       " 'Pet_Supplies.pkl',\n",
       " 'Automotive.pkl',\n",
       " 'Tools_and_Home_Improvement.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = os.listdir('/data/liufengyuan/NLPinFinance/Unziped_Filtered_Data')\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_read = open('/data/liufengyuan/NLPinFinance/Unziped_Filtered_Data/Clothing_Shoes_and_Jewelry.pkl', 'rb')\n",
    "All_data = pickle.load(f_read)\n",
    "f_read.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Pantone Accuracy and Convenience for Four-Color Printing. The new PLUS SERIES CMYK guides contain 2,868 colors, chromatically arranged for smoother transitions between colors and more intuitive selection. These guides are an ideal way to visualize, communicate and control colors for type, logos, borders, backgrounds and other graphics treatments. Compatible with today's digital workflows. Screen tint percentages are provided to enable accurate color reproduction. Printed using bio-friendly, ISO-certified inks. Text weight paper is used, to match popular print specifications. Includes color index, lighting evaluation tool and design software. Key product benefits: 1) Accurately select, specify and communicate color for four-color process printing; 2) Easy location of desired colors; 3) Easily reproduce colors in process printing with supplied CMYK screen percentages; 4) Shows the effect of coated and uncoated stocks on each color; 5) Provides a printed standard for color comparison and quality control; 6) All colors are reproducible within commercially accepted printing standards.\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_data[3]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell_Phones_and_Accessories.pkl: (total length: 282198) has mean title length 129.80852518967112, mean feature length 472.4708875651579, mean description length 660.0881044936375\n",
      "Office_Products.pkl: (total length: 135100) has mean title length 87.18961369641971, mean feature length 401.0041598507783, mean description length 580.8472994278354\n",
      "Movies_and_TV.pkl: (total length: 53) has mean title length 63.148148148148145, mean feature length 338.037037037037, mean description length 901.7962962962963\n",
      "Patio_Lawn_and_Garden.pkl: (total length: 112378) has mean title length 81.9882362363075, mean feature length 429.0063179063704, mean description length 667.7810178058178\n",
      "Clothing_Shoes_and_Jewelry.pkl: (total length: 1513427) has mean title length 621.0240711814503, mean feature length 366.00089597919424, mean description length 516.8315830022967\n",
      "Electronics.pkl: (total length: 420725) has mean title length 108.96757509638101, mean feature length 484.1350546436398, mean description length 880.4408498642822\n",
      "Industrial_and_Scientific.pkl: (total length: 25819) has mean title length 88.29918667699458, mean feature length 394.8879163439194, mean description length 713.4946553059643\n",
      "Kindle_Store.pkl: (total length: 0) has mean title length 0.0, mean feature length 0.0, mean description length 0.0\n",
      "Digital_Music.pkl: (total length: 23) has mean title length 68.20833333333333, mean feature length 128.45833333333334, mean description length 258.625\n",
      "Home_and_Kitchen.pkl: (total length: 536316) has mean title length 83.04058980043519, mean feature length 430.47090806370113, mean description length 581.5831457887779\n",
      "Sports_and_Outdoors.pkl: (total length: 365763) has mean title length 68.03204798722673, mean feature length 822.8115232773044, mean description length 576.3706761737077\n",
      "CDs_and_Vinyl.pkl: (total length: 71) has mean title length 45.013888888888886, mean feature length 243.13888888888889, mean description length 483.18055555555554\n",
      "Books.pkl: (total length: 966) has mean title length 60.82626680455016, mean feature length 312.7776628748707, mean description length 748.9462254395036\n",
      "Arts_Crafts_and_Sewing.pkl: (total length: 112253) has mean title length 80.39289468526734, mean feature length 433.55628307231814, mean description length 494.50884600994175\n",
      "Toys_and_Games.pkl: (total length: 242439) has mean title length 66.3300156739812, mean feature length 352.1482098663587, mean description length 549.5815253258538\n",
      "Grocery_and_Gourmet_Food.pkl: (total length: 8209) has mean title length 76.40353227771011, mean feature length 467.18172959805116, mean description length 743.0455542021924\n",
      "AMAZON_FASHION.pkl: (total length: 8310) has mean title length 610.9311755504752, mean feature length 424.1798820839851, mean description length 484.73216219468173\n",
      "Pet_Supplies.pkl: (total length: 108456) has mean title length 72.56409452594116, mean feature length 406.7612786634334, mean description length 601.724351586343\n",
      "Automotive.pkl: (total length: 418846) has mean title length 78.71878514111359, mean feature length 379.6169508197504, mean description length 725.1025195357732\n",
      "Tools_and_Home_Improvement.pkl: (total length: 236198) has mean title length 87.55753411318422, mean feature length 403.70275064670045, mean description length 737.9493393282783\n"
     ]
    }
   ],
   "source": [
    "for file in filenames:\n",
    "    f_read = open('/data/liufengyuan/NLPinFinance/Unziped_Filtered_Data/' + file, 'rb')\n",
    "    All_data = pickle.load(f_read)\n",
    "    f_read.close()\n",
    "    temp_dict = {}\n",
    "\n",
    "    tot_cap_len = 0\n",
    "    tot_fea_len = 0\n",
    "    tot_des_len = 0\n",
    "    for idx, item in enumerate(All_data):\n",
    "        tot_cap_len += len(item['title'])\n",
    "        for fea in item['feature']:\n",
    "            tot_fea_len += len(fea)\n",
    "        for fea in item['description']:\n",
    "            tot_des_len += len(fea)\n",
    "    print(f\"{file}: (total length: {len(All_data)}) has mean title length {tot_cap_len/(1+len(All_data))}, mean feature length {tot_fea_len/(1+len(All_data))}, mean description length {tot_des_len/(1+len(All_data))}\")\n",
    "    temp_dict['length'] = len(All_data)\n",
    "    temp_dict['title'] = tot_cap_len/(1+len(All_data))\n",
    "    temp_dict['feature'] = tot_fea_len/(1+len(All_data))\n",
    "    temp_dict['description'] = tot_des_len/(1+len(All_data))\n",
    "    file_dict[file[:-4]] = temp_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_file_dict = file_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cell_Phones_and_Accessories': {'length': 282198,\n",
       "  'title': 129.80852518967112,\n",
       "  'feature': 472.4708875651579,\n",
       "  'description': 660.0881044936375},\n",
       " 'Office_Products': {'length': 135100,\n",
       "  'title': 87.18961369641971,\n",
       "  'feature': 401.0041598507783,\n",
       "  'description': 580.8472994278354},\n",
       " 'Movies_and_TV': {'length': 53,\n",
       "  'title': 63.148148148148145,\n",
       "  'feature': 338.037037037037,\n",
       "  'description': 901.7962962962963},\n",
       " 'Patio_Lawn_and_Garden': {'length': 112378,\n",
       "  'title': 81.9882362363075,\n",
       "  'feature': 429.0063179063704,\n",
       "  'description': 667.7810178058178},\n",
       " 'Clothing_Shoes_and_Jewelry': {'length': 1513427,\n",
       "  'title': 621.0240711814503,\n",
       "  'feature': 366.00089597919424,\n",
       "  'description': 516.8315830022967},\n",
       " 'Electronics': {'length': 420725,\n",
       "  'title': 108.96757509638101,\n",
       "  'feature': 484.1350546436398,\n",
       "  'description': 880.4408498642822},\n",
       " 'Industrial_and_Scientific': {'length': 25819,\n",
       "  'title': 88.29918667699458,\n",
       "  'feature': 394.8879163439194,\n",
       "  'description': 713.4946553059643},\n",
       " 'Kindle_Store': {'length': 0,\n",
       "  'title': 0.0,\n",
       "  'feature': 0.0,\n",
       "  'description': 0.0},\n",
       " 'Digital_Music': {'length': 23,\n",
       "  'title': 68.20833333333333,\n",
       "  'feature': 128.45833333333334,\n",
       "  'description': 258.625},\n",
       " 'Home_and_Kitchen': {'length': 536316,\n",
       "  'title': 83.04058980043519,\n",
       "  'feature': 430.47090806370113,\n",
       "  'description': 581.5831457887779},\n",
       " 'Sports_and_Outdoors': {'length': 365763,\n",
       "  'title': 68.03204798722673,\n",
       "  'feature': 822.8115232773044,\n",
       "  'description': 576.3706761737077},\n",
       " 'CDs_and_Vinyl': {'length': 71,\n",
       "  'title': 45.013888888888886,\n",
       "  'feature': 243.13888888888889,\n",
       "  'description': 483.18055555555554},\n",
       " 'Books': {'length': 966,\n",
       "  'title': 60.82626680455016,\n",
       "  'feature': 312.7776628748707,\n",
       "  'description': 748.9462254395036},\n",
       " 'Arts_Crafts_and_Sewing': {'length': 112253,\n",
       "  'title': 80.39289468526734,\n",
       "  'feature': 433.55628307231814,\n",
       "  'description': 494.50884600994175},\n",
       " 'Toys_and_Games': {'length': 242439,\n",
       "  'title': 66.3300156739812,\n",
       "  'feature': 352.1482098663587,\n",
       "  'description': 549.5815253258538},\n",
       " 'Grocery_and_Gourmet_Food': {'length': 8209,\n",
       "  'title': 76.40353227771011,\n",
       "  'feature': 467.18172959805116,\n",
       "  'description': 743.0455542021924},\n",
       " 'AMAZON_FASHION': {'length': 8310,\n",
       "  'title': 610.9311755504752,\n",
       "  'feature': 424.1798820839851,\n",
       "  'description': 484.73216219468173},\n",
       " 'Pet_Supplies': {'length': 108456,\n",
       "  'title': 72.56409452594116,\n",
       "  'feature': 406.7612786634334,\n",
       "  'description': 601.724351586343},\n",
       " 'Automotive': {'length': 418846,\n",
       "  'title': 78.71878514111359,\n",
       "  'feature': 379.6169508197504,\n",
       "  'description': 725.1025195357732},\n",
       " 'Tools_and_Home_Improvement': {'length': 236198,\n",
       "  'title': 87.55753411318422,\n",
       "  'feature': 403.70275064670045,\n",
       "  'description': 737.9493393282783}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in filenames:\n",
    "    f_read = open('/data/liufengyuan/NLPinFinance/Unziped_Filtered_Data/' + file, 'rb')\n",
    "    All_data = pickle.load(f_read)\n",
    "    f_read.close()\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for idx, item in enumerate(All_data):\n",
    "\n",
    "        tot_cap_len = len(item['title'])\n",
    "\n",
    "        if tot_cap_len < file_dict[file[:-4]]['title']:\n",
    "            tot_fea_len = 0\n",
    "            for fea in item['feature']:\n",
    "                tot_fea_len += len(fea)\n",
    "            \n",
    "            if tot_fea_len < file_dict[file[:-4]]['feature']:\n",
    "                tot_des_len = 0\n",
    "                for des in item['description']:\n",
    "                    tot_des_len += len(des)\n",
    "\n",
    "                if tot_des_len < file_dict[file[:-4]]['description']:\n",
    "                    count+=1\n",
    "    file_dict[file[:-4]]['length'] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cell_Phones_and_Accessories': {'length': 95188,\n",
       "  'title': 129.80852518967112,\n",
       "  'feature': 472.4708875651579,\n",
       "  'description': 660.0881044936375},\n",
       " 'Office_Products': {'length': 50768,\n",
       "  'title': 87.18961369641971,\n",
       "  'feature': 401.0041598507783,\n",
       "  'description': 580.8472994278354},\n",
       " 'Movies_and_TV': {'length': 14,\n",
       "  'title': 63.148148148148145,\n",
       "  'feature': 338.037037037037,\n",
       "  'description': 901.7962962962963},\n",
       " 'Patio_Lawn_and_Garden': {'length': 38839,\n",
       "  'title': 81.9882362363075,\n",
       "  'feature': 429.0063179063704,\n",
       "  'description': 667.7810178058178},\n",
       " 'Clothing_Shoes_and_Jewelry': {'length': 768609,\n",
       "  'title': 621.0240711814503,\n",
       "  'feature': 366.00089597919424,\n",
       "  'description': 516.8315830022967},\n",
       " 'Electronics': {'length': 149181,\n",
       "  'title': 108.96757509638101,\n",
       "  'feature': 484.1350546436398,\n",
       "  'description': 880.4408498642822},\n",
       " 'Industrial_and_Scientific': {'length': 9302,\n",
       "  'title': 88.29918667699458,\n",
       "  'feature': 394.8879163439194,\n",
       "  'description': 713.4946553059643},\n",
       " 'Kindle_Store': {'length': 0,\n",
       "  'title': 0.0,\n",
       "  'feature': 0.0,\n",
       "  'description': 0.0},\n",
       " 'Digital_Music': {'length': 8,\n",
       "  'title': 68.20833333333333,\n",
       "  'feature': 128.45833333333334,\n",
       "  'description': 258.625},\n",
       " 'Home_and_Kitchen': {'length': 198095,\n",
       "  'title': 83.04058980043519,\n",
       "  'feature': 430.47090806370113,\n",
       "  'description': 581.5831457887779},\n",
       " 'Sports_and_Outdoors': {'length': 137210,\n",
       "  'title': 68.03204798722673,\n",
       "  'feature': 822.8115232773044,\n",
       "  'description': 576.3706761737077},\n",
       " 'CDs_and_Vinyl': {'length': 18,\n",
       "  'title': 45.013888888888886,\n",
       "  'feature': 243.13888888888889,\n",
       "  'description': 483.18055555555554},\n",
       " 'Books': {'length': 264,\n",
       "  'title': 60.82626680455016,\n",
       "  'feature': 312.7776628748707,\n",
       "  'description': 748.9462254395036},\n",
       " 'Arts_Crafts_and_Sewing': {'length': 40406,\n",
       "  'title': 80.39289468526734,\n",
       "  'feature': 433.55628307231814,\n",
       "  'description': 494.50884600994175},\n",
       " 'Toys_and_Games': {'length': 80610,\n",
       "  'title': 66.3300156739812,\n",
       "  'feature': 352.1482098663587,\n",
       "  'description': 549.5815253258538},\n",
       " 'Grocery_and_Gourmet_Food': {'length': 2554,\n",
       "  'title': 76.40353227771011,\n",
       "  'feature': 467.18172959805116,\n",
       "  'description': 743.0455542021924},\n",
       " 'AMAZON_FASHION': {'length': 3646,\n",
       "  'title': 610.9311755504752,\n",
       "  'feature': 424.1798820839851,\n",
       "  'description': 484.73216219468173},\n",
       " 'Pet_Supplies': {'length': 42060,\n",
       "  'title': 72.56409452594116,\n",
       "  'feature': 406.7612786634334,\n",
       "  'description': 601.724351586343},\n",
       " 'Automotive': {'length': 143482,\n",
       "  'title': 78.71878514111359,\n",
       "  'feature': 379.6169508197504,\n",
       "  'description': 725.1025195357732},\n",
       " 'Tools_and_Home_Improvement': {'length': 84358,\n",
       "  'title': 87.55753411318422,\n",
       "  'feature': 403.70275064670045,\n",
       "  'description': 737.9493393282783}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Clothing_Shoes_and_Jewelry',\n",
       "  {'length': 768609,\n",
       "   'title': 621.0240711814503,\n",
       "   'feature': 366.00089597919424,\n",
       "   'description': 516.8315830022967}),\n",
       " ('Home_and_Kitchen',\n",
       "  {'length': 198095,\n",
       "   'title': 83.04058980043519,\n",
       "   'feature': 430.47090806370113,\n",
       "   'description': 581.5831457887779}),\n",
       " ('Electronics',\n",
       "  {'length': 149181,\n",
       "   'title': 108.96757509638101,\n",
       "   'feature': 484.1350546436398,\n",
       "   'description': 880.4408498642822}),\n",
       " ('Automotive',\n",
       "  {'length': 143482,\n",
       "   'title': 78.71878514111359,\n",
       "   'feature': 379.6169508197504,\n",
       "   'description': 725.1025195357732}),\n",
       " ('Sports_and_Outdoors',\n",
       "  {'length': 137210,\n",
       "   'title': 68.03204798722673,\n",
       "   'feature': 822.8115232773044,\n",
       "   'description': 576.3706761737077}),\n",
       " ('Cell_Phones_and_Accessories',\n",
       "  {'length': 95188,\n",
       "   'title': 129.80852518967112,\n",
       "   'feature': 472.4708875651579,\n",
       "   'description': 660.0881044936375}),\n",
       " ('Tools_and_Home_Improvement',\n",
       "  {'length': 84358,\n",
       "   'title': 87.55753411318422,\n",
       "   'feature': 403.70275064670045,\n",
       "   'description': 737.9493393282783}),\n",
       " ('Toys_and_Games',\n",
       "  {'length': 80610,\n",
       "   'title': 66.3300156739812,\n",
       "   'feature': 352.1482098663587,\n",
       "   'description': 549.5815253258538}),\n",
       " ('Office_Products',\n",
       "  {'length': 50768,\n",
       "   'title': 87.18961369641971,\n",
       "   'feature': 401.0041598507783,\n",
       "   'description': 580.8472994278354}),\n",
       " ('Pet_Supplies',\n",
       "  {'length': 42060,\n",
       "   'title': 72.56409452594116,\n",
       "   'feature': 406.7612786634334,\n",
       "   'description': 601.724351586343}),\n",
       " ('Arts_Crafts_and_Sewing',\n",
       "  {'length': 40406,\n",
       "   'title': 80.39289468526734,\n",
       "   'feature': 433.55628307231814,\n",
       "   'description': 494.50884600994175}),\n",
       " ('Patio_Lawn_and_Garden',\n",
       "  {'length': 38839,\n",
       "   'title': 81.9882362363075,\n",
       "   'feature': 429.0063179063704,\n",
       "   'description': 667.7810178058178}),\n",
       " ('Industrial_and_Scientific',\n",
       "  {'length': 9302,\n",
       "   'title': 88.29918667699458,\n",
       "   'feature': 394.8879163439194,\n",
       "   'description': 713.4946553059643}),\n",
       " ('AMAZON_FASHION',\n",
       "  {'length': 3646,\n",
       "   'title': 610.9311755504752,\n",
       "   'feature': 424.1798820839851,\n",
       "   'description': 484.73216219468173}),\n",
       " ('Grocery_and_Gourmet_Food',\n",
       "  {'length': 2554,\n",
       "   'title': 76.40353227771011,\n",
       "   'feature': 467.18172959805116,\n",
       "   'description': 743.0455542021924}),\n",
       " ('Books',\n",
       "  {'length': 264,\n",
       "   'title': 60.82626680455016,\n",
       "   'feature': 312.7776628748707,\n",
       "   'description': 748.9462254395036}),\n",
       " ('CDs_and_Vinyl',\n",
       "  {'length': 18,\n",
       "   'title': 45.013888888888886,\n",
       "   'feature': 243.13888888888889,\n",
       "   'description': 483.18055555555554}),\n",
       " ('Movies_and_TV',\n",
       "  {'length': 14,\n",
       "   'title': 63.148148148148145,\n",
       "   'feature': 338.037037037037,\n",
       "   'description': 901.7962962962963}),\n",
       " ('Digital_Music',\n",
       "  {'length': 8,\n",
       "   'title': 68.20833333333333,\n",
       "   'feature': 128.45833333333334,\n",
       "   'description': 258.625}),\n",
       " ('Kindle_Store',\n",
       "  {'length': 0, 'title': 0.0, 'feature': 0.0, 'description': 0.0})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(file_dict.items(), key = lambda x: x[1]['length'], reverse=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top five files\n",
    "\n",
    "* 'Clothing_Shoes_and_Jewelry',\n",
    "* 'Home_and_Kitchen',\n",
    "* 'Electronics',\n",
    "* 'Automotive',\n",
    "* 'Sports_and_Outdoors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file_name = ['Clothing_Shoes_and_Jewelry.pkl', 'Home_and_Kitchen.pkl', 'Electronics.pkl', 'Automotive.pkl', 'Sports_and_Outdoors.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save file: Clothing_Shoes_and_Jewelry\n",
      "Save file: Home_and_Kitchen\n",
      "Save file: Electronics\n",
      "Save file: Automotive\n",
      "Save file: Sports_and_Outdoors\n"
     ]
    }
   ],
   "source": [
    "for file in new_file_name:\n",
    "    f_read = open('/data/liufengyuan/NLPinFinance/Unziped_Filtered_Data/' + file, 'rb')\n",
    "    All_data = pickle.load(f_read)\n",
    "    f_read.close()\n",
    "\n",
    "    new_file = []\n",
    "\n",
    "    for idx, item in enumerate(All_data):\n",
    "\n",
    "        tot_cap_len = len(item['title'])\n",
    "\n",
    "        if tot_cap_len < file_dict[file[:-4]]['title']:\n",
    "            tot_fea_len = 0\n",
    "            for fea in item['feature']:\n",
    "                tot_fea_len += len(fea)\n",
    "            \n",
    "            if tot_fea_len < file_dict[file[:-4]]['feature']:\n",
    "                tot_des_len = 0\n",
    "                for des in item['description']:\n",
    "                    tot_des_len += len(des)\n",
    "\n",
    "                if tot_des_len < file_dict[file[:-4]]['description']:\n",
    "                    new_file.append(item)\n",
    "    print('Save file: ' + file[:-4])\n",
    "    json.dump(new_file, open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + file[:-4] + '.json', 'w'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match data to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = os.listdir('/data/liufengyuan/NLPinFinance/trainvalImage/train')\n",
    "test = os.listdir('/data/liufengyuan/NLPinFinance/trainvalImage/test')\n",
    "val = os.listdir('/data/liufengyuan/NLPinFinance/trainvalImage/val')\n",
    "\n",
    "def select_data(item, new_data_val, new_data_test, new_data_train):\n",
    "    id = item['asin'] + '.jpg'\n",
    "    if id in val:\n",
    "        new_data_val.append(item)\n",
    "    elif id in test:\n",
    "        new_data_test.append(item)\n",
    "    elif id in train:\n",
    "        new_data_train.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of round is 16\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 0 : new_data_val is 571, new_data_test is 502, new_data_train is 47702\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 1 : new_data_val is 1121, new_data_test is 1039, new_data_train is 95776\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 2 : new_data_val is 1615, new_data_test is 1575, new_data_train is 143905\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 3 : new_data_val is 2180, new_data_test is 2118, new_data_train is 191765\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 4 : new_data_val is 2716, new_data_test is 2670, new_data_train is 237946\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 5 : new_data_val is 3198, new_data_test is 3185, new_data_train is 280655\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 6 : new_data_val is 3684, new_data_test is 3685, new_data_train is 323499\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 7 : new_data_val is 4140, new_data_test is 4167, new_data_train is 367599\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 8 : new_data_val is 4647, new_data_test is 4691, new_data_train is 411079\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 9 : new_data_val is 5145, new_data_test is 5153, new_data_train is 454580\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 10 : new_data_val is 5643, new_data_test is 5678, new_data_train is 499066\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 11 : new_data_val is 6134, new_data_test is 6131, new_data_train is 541544\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 12 : new_data_val is 6595, new_data_test is 6577, new_data_train is 581078\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 13 : new_data_val is 7079, new_data_test is 7031, new_data_train is 621406\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 14 : new_data_val is 7553, new_data_test is 7505, new_data_train is 662487\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 15 : new_data_val is 7735, new_data_test is 7673, new_data_train is 677457\n",
      "Save new_data_val\n",
      "Save new_data_test\n",
      "Save new_data_train\n",
      "Total number of round is 4\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 0 : new_data_val is 576, new_data_test is 573, new_data_train is 48764\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 1 : new_data_val is 1128, new_data_test is 1134, new_data_train is 97645\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 2 : new_data_val is 1704, new_data_test is 1665, new_data_train is 146535\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 3 : new_data_val is 2264, new_data_test is 2228, new_data_train is 193502\n",
      "Save new_data_val\n",
      "Save new_data_test\n",
      "Save new_data_train\n",
      "Total number of round is 3\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 0 : new_data_val is 515, new_data_test is 581, new_data_train is 48891\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 1 : new_data_val is 1051, new_data_test is 1145, new_data_train is 97789\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 2 : new_data_val is 1612, new_data_test is 1679, new_data_train is 145873\n",
      "Save new_data_val\n",
      "Save new_data_test\n",
      "Save new_data_train\n",
      "Total number of round is 3\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 0 : new_data_val is 528, new_data_test is 572, new_data_train is 48893\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 1 : new_data_val is 1118, new_data_test is 1150, new_data_train is 97716\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 2 : new_data_val is 1625, new_data_test is 1640, new_data_train is 140195\n",
      "Save new_data_val\n",
      "Save new_data_test\n",
      "Save new_data_train\n",
      "Total number of round is 3\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 0 : new_data_val is 542, new_data_test is 576, new_data_train is 48214\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "[38000/50000] Tokenized the captions.\n",
      "[40000/50000] Tokenized the captions.\n",
      "[42000/50000] Tokenized the captions.\n",
      "[44000/50000] Tokenized the captions.\n",
      "[46000/50000] Tokenized the captions.\n",
      "[48000/50000] Tokenized the captions.\n",
      "[50000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 1 : new_data_val is 1015, new_data_test is 1096, new_data_train is 90507\n",
      "[2000/50000] Tokenized the captions.\n",
      "[4000/50000] Tokenized the captions.\n",
      "[6000/50000] Tokenized the captions.\n",
      "[8000/50000] Tokenized the captions.\n",
      "[10000/50000] Tokenized the captions.\n",
      "[12000/50000] Tokenized the captions.\n",
      "[14000/50000] Tokenized the captions.\n",
      "[16000/50000] Tokenized the captions.\n",
      "[18000/50000] Tokenized the captions.\n",
      "[20000/50000] Tokenized the captions.\n",
      "[22000/50000] Tokenized the captions.\n",
      "[24000/50000] Tokenized the captions.\n",
      "[26000/50000] Tokenized the captions.\n",
      "[28000/50000] Tokenized the captions.\n",
      "[30000/50000] Tokenized the captions.\n",
      "[32000/50000] Tokenized the captions.\n",
      "[34000/50000] Tokenized the captions.\n",
      "[36000/50000] Tokenized the captions.\n",
      "Close pool\n",
      "Join pool\n",
      "Round 2 : new_data_val is 1359, new_data_test is 1458, new_data_train is 121227\n",
      "Save new_data_val\n",
      "Save new_data_test\n",
      "Save new_data_train\n"
     ]
    }
   ],
   "source": [
    "for file in new_file_name:\n",
    "    f_save = open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + file[:-4] + '.json', 'r')\n",
    "    total_data = json.load(f_save)\n",
    "    f_save.close()\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    new_data_val = manager.list()\n",
    "    new_data_test = manager.list()\n",
    "    new_data_train = manager.list()\n",
    "\n",
    "    total_data_length = len(total_data)\n",
    "    each_step = 50000\n",
    "    print(f'Total number of round is {math.ceil(total_data_length/each_step)}')\n",
    "    for i in range(math.ceil(total_data_length/each_step)):\n",
    "        pool = mp.Pool(48)\n",
    "        count = 0\n",
    "        start = each_step*i\n",
    "        end = each_step*(i+1)\n",
    "        if end > total_data_length:\n",
    "            end = total_data_length\n",
    "        for item in total_data[start:end]:\n",
    "            count += 1\n",
    "            if count % 2000 == 0:\n",
    "                print(\"[%d/%d] Tokenized the captions.\" %(count, each_step))\n",
    "            pool.apply_async(select_data, args = (item, new_data_val, new_data_test, new_data_train))\n",
    "        print('Close pool')\n",
    "        pool.close()\n",
    "        print('Join pool')\n",
    "        pool.join()\n",
    "        print(f'Round {i} : new_data_val is {len(new_data_val)}, new_data_test is {len(new_data_test)}, new_data_train is {len(new_data_train)}')\n",
    "\n",
    "    print('Save new_data_val')\n",
    "    f_save = open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + file[:-4] + '/' + file[:-4] + '_val.json', 'w')\n",
    "    json.dump(list(new_data_val), f_save)\n",
    "    f_save.close()\n",
    "\n",
    "    print('Save new_data_test')\n",
    "    f_save = open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + file[:-4] + '/' + file[:-4] + '_test.json', 'w')\n",
    "    json.dump(list(new_data_test), f_save)\n",
    "    f_save.close()\n",
    "\n",
    "    print('Save new_data_train')\n",
    "    f_save = open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + file[:-4] + '/' + file[:-4] + '_train.json', 'w')\n",
    "    json.dump(list(new_data_train), f_save)\n",
    "    f_save.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change train, test, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothing_Shoes_and_Jewelry\n",
      "672865 10000 10000\n",
      "692865\n",
      "Save new_data_val\n",
      "Save new_data_test\n",
      "Save new_data_train\n",
      "Clothing_Shoes_and_Jewelry's train: 662865 test: 15000 val: 15000\n",
      "Home_and_Kitchen\n",
      "177994 10000 10000\n",
      "197994\n",
      "Save new_data_val\n",
      "Save new_data_test\n",
      "Save new_data_train\n",
      "Home_and_Kitchen's train: 177994 test: 10000 val: 10000\n",
      "Electronics\n",
      "129164 10000 10000\n",
      "149164\n",
      "Save new_data_val\n",
      "Save new_data_test\n",
      "Save new_data_train\n",
      "Electronics's train: 139164 test: 5000 val: 5000\n",
      "Automotive\n",
      "123460 10000 10000\n",
      "143460\n",
      "Save new_data_val\n",
      "Save new_data_test\n",
      "Save new_data_train\n",
      "Automotive's train: 133460 test: 5000 val: 5000\n",
      "Sports_and_Outdoors\n",
      "104044 10000 10000\n",
      "124044\n",
      "Save new_data_val\n",
      "Save new_data_test\n",
      "Save new_data_train\n",
      "Sports_and_Outdoors's train: 114044 test: 5000 val: 5000\n"
     ]
    }
   ],
   "source": [
    "for file in new_file_name:\n",
    "    this_file = file[:-4]\n",
    "    print(this_file)\n",
    "    data_train = json.load(open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/' + this_file + '_train.json', 'r'))\n",
    "    data_test = json.load(open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/' + this_file + '_test.json', 'r'))\n",
    "    data_val = json.load(open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/' + this_file + '_val.json', 'r'))\n",
    "    print(len(data_train), len(data_test), len(data_val))\n",
    "\n",
    "\n",
    "    total_dict = []\n",
    "    total_dict.extend(data_train)\n",
    "    total_dict.extend(data_test)\n",
    "    total_dict.extend(data_val)\n",
    "    print(len(total_dict))\n",
    "\n",
    "    shuffle(total_dict)\n",
    "\n",
    "    num_val_test = 10000\n",
    "    if len(total_dict) > 300000:\n",
    "        num_val_test = 15000\n",
    "    elif len(total_dict) < 150000:\n",
    "        num_val_test = 5000\n",
    "    new_data_val = total_dict[:num_val_test]\n",
    "    new_data_test= total_dict[num_val_test:2*num_val_test]\n",
    "    new_data_train = total_dict[2*num_val_test:]\n",
    "    \n",
    "\n",
    "    print('Save new_data_val')\n",
    "    f_save = open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/' + this_file + '_val.json', 'w')\n",
    "    json.dump(new_data_val, f_save)\n",
    "    f_save.close()\n",
    "\n",
    "    print('Save new_data_test')\n",
    "    f_save = open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/' + this_file + '_test.json', 'w')\n",
    "    json.dump(new_data_test, f_save)\n",
    "    f_save.close()\n",
    "\n",
    "    print('Save new_data_train')\n",
    "    f_save = open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/' + this_file + '_train.json', 'w')\n",
    "    json.dump(new_data_train, f_save)\n",
    "    f_save.close()\n",
    "\n",
    "    print(f\"{this_file}'s train: {len(new_data_train)} test: {len(new_data_test)} val: {len(new_data_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data formet to COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'val', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothing_Shoes_and_Jewelry\n",
      "Original length is 662865.  Output length is 626923. 94.57777978924818%\n",
      "Original length is 15000.  Output length is 14186. 94.57333333333334%\n",
      "Original length is 15000.  Output length is 14211. 94.74000000000001%\n",
      "Home_and_Kitchen\n",
      "Original length is 177994.  Output length is 173150. 97.27855995145904%\n",
      "Original length is 10000.  Output length is 9715. 97.15%\n",
      "Original length is 10000.  Output length is 9727. 97.27%\n",
      "Electronics\n",
      "Original length is 139164.  Output length is 126635. 90.99695323503205%\n",
      "Original length is 5000.  Output length is 4534. 90.68%\n",
      "Original length is 5000.  Output length is 4561. 91.22%\n",
      "Automotive\n",
      "Original length is 133460.  Output length is 128212. 96.06773565113143%\n",
      "Original length is 5000.  Output length is 4815. 96.3%\n",
      "Original length is 5000.  Output length is 4814. 96.28%\n",
      "Sports_and_Outdoors\n",
      "Original length is 114044.  Output length is 110069. 96.51450317421346%\n",
      "Original length is 5000.  Output length is 4806. 96.12%\n",
      "Original length is 5000.  Output length is 4803. 96.06%\n"
     ]
    }
   ],
   "source": [
    "for file in new_file_name:\n",
    "    this_file = file[:-4]\n",
    "    print(this_file)\n",
    "    for split in splits:\n",
    "        val_data = json.load(open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/' + this_file + '_' + split + '.json', 'r'))\n",
    "        val_dict = {'type': 'caption', 'images':[], 'annotations': []}\n",
    "        val_len = len(val_data)\n",
    "        count = 0\n",
    "        for idx, item in enumerate(val_data):\n",
    "            if len(item['description']) == 1:\n",
    "                temp_len = 0\n",
    "                temp_anno = {}\n",
    "                temp_anno['image_id'] = item['asin']\n",
    "                temp_anno['id'] = idx + 3058\n",
    "                temp_anno['caption'] = item['title']\n",
    "                temp_anno['feature'] = item['feature']\n",
    "                temp_anno['description'] = item['description'][0]\n",
    "                val_dict['annotations'].append(temp_anno)\n",
    "\n",
    "                temp_img = {}\n",
    "                temp_img['license'] = random.randint(0, 9)\n",
    "                temp_img['file_name'] = item['asin'] + '.jpg'\n",
    "                temp_img['id'] = item['asin']\n",
    "                val_dict['images'].append(temp_img)\n",
    "        json.dump(val_dict, open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/annotations/' + 'karpathy_split_' + split + '.json', 'w'))\n",
    "        print(f\"Original length is {val_len}.  Output length is {len(val_dict['images'])}. {len(val_dict['images'])/val_len * 100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothing_Shoes_and_Jewelry\n",
      "Average description length of Clothing_Shoes_and_Jewelry in train set is 215.4397924466003\n",
      "Average description length of Clothing_Shoes_and_Jewelry in val set is 216.6265332017482\n",
      "Average description length of Clothing_Shoes_and_Jewelry in test set is 213.691295475336\n",
      "Home_and_Kitchen\n",
      "Average description length of Home_and_Kitchen in train set is 265.0142881894311\n",
      "Average description length of Home_and_Kitchen in val set is 265.46392177045806\n",
      "Average description length of Home_and_Kitchen in test set is 266.53613652719235\n",
      "Electronics\n",
      "Average description length of Electronics in train set is 338.43957831563154\n",
      "Average description length of Electronics in val set is 340.79841199823557\n",
      "Average description length of Electronics in test set is 338.1394431045823\n",
      "Automotive\n",
      "Average description length of Automotive in train set is 284.44951330608677\n",
      "Average description length of Automotive in val set is 283.4753894080997\n",
      "Average description length of Automotive in test set is 281.3666389696718\n",
      "Sports_and_Outdoors\n",
      "Average description length of Sports_and_Outdoors in train set is 259.8383559403647\n",
      "Average description length of Sports_and_Outdoors in val set is 260.4700374531835\n",
      "Average description length of Sports_and_Outdoors in test set is 263.82615032271497\n"
     ]
    }
   ],
   "source": [
    "for file in new_file_name:\n",
    "    this_file = file[:-4]\n",
    "    print(this_file)\n",
    "    for split in splits:\n",
    "        val_data = json.load(open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/annotations/karpathy_split_' + split + '.json', 'r'))\n",
    "        val_len = len(val_data['annotations'])\n",
    "        total_length = 0\n",
    "        for item in val_data['annotations']:\n",
    "            total_length += len(item['description'])\n",
    "        \n",
    "        print(f\"Average description length of {this_file} in {split} set is {total_length/val_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothing_Shoes_and_Jewelry\n",
      "Original image length is 626923.  Output length is 626923. 100.0%\n",
      "Original annotation length is 626923.  Output length is 3725292. 594.2184287384575%\n",
      "Original image length is 14186.  Output length is 14186. 100.0%\n",
      "Original annotation length is 14186.  Output length is 84294. 594.2055547723107%\n",
      "Original image length is 14211.  Output length is 14211. 100.0%\n",
      "Original annotation length is 14211.  Output length is 84771. 596.5167827739076%\n",
      "Home_and_Kitchen\n",
      "Original image length is 173150.  Output length is 173150. 100.0%\n",
      "Original annotation length is 173150.  Output length is 743763. 429.54836846664745%\n",
      "Original image length is 9715.  Output length is 9715. 100.0%\n",
      "Original annotation length is 9715.  Output length is 41793. 430.19042717447246%\n",
      "Original image length is 9727.  Output length is 9727. 100.0%\n",
      "Original annotation length is 9727.  Output length is 41615. 427.829752236044%\n",
      "Electronics\n",
      "Original image length is 126635.  Output length is 126635. 100.0%\n",
      "Original annotation length is 126635.  Output length is 527575. 416.610731630276%\n",
      "Original image length is 4534.  Output length is 4534. 100.0%\n",
      "Original annotation length is 4534.  Output length is 18784. 414.2920158800177%\n",
      "Original image length is 4561.  Output length is 4561. 100.0%\n",
      "Original annotation length is 4561.  Output length is 18928. 414.9967112475334%\n",
      "Automotive\n",
      "Original image length is 128212.  Output length is 128212. 100.0%\n",
      "Original annotation length is 128212.  Output length is 472565. 368.58094406139827%\n",
      "Original image length is 4815.  Output length is 4815. 100.0%\n",
      "Original annotation length is 4815.  Output length is 17892. 371.58878504672896%\n",
      "Original image length is 4814.  Output length is 4814. 100.0%\n",
      "Original annotation length is 4814.  Output length is 17879. 371.3959285417532%\n",
      "Sports_and_Outdoors\n",
      "Original image length is 110069.  Output length is 110069. 100.0%\n",
      "Original annotation length is 110069.  Output length is 490185. 445.3433755189926%\n",
      "Original image length is 4806.  Output length is 4806. 100.0%\n",
      "Original annotation length is 4806.  Output length is 21415. 445.5888472742405%\n",
      "Original image length is 4803.  Output length is 4803. 100.0%\n",
      "Original annotation length is 4803.  Output length is 21471. 447.0331043098064%\n"
     ]
    }
   ],
   "source": [
    "for file in new_file_name:\n",
    "    this_file = file[:-4]\n",
    "    print(this_file)\n",
    "    for split in splits:\n",
    "        val_data = json.load(open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/annotations/karpathy_split_' + split + '.json', 'r'))\n",
    "        val_dict = {'type': 'caption', 'images':[], 'annotations': []}\n",
    "        val_len = len(val_data['annotations'])\n",
    "        count = 0\n",
    "\n",
    "        \n",
    "        for item in val_data['annotations']:\n",
    "            temp_len = 0\n",
    "            temp_anno = {}\n",
    "            temp_anno['image_id'] = item['image_id']\n",
    "            temp_anno['id'] = item['id']\n",
    "            temp_anno['caption'] = item['caption']\n",
    "            temp_anno['description'] = item['description']\n",
    "            for fea in item['feature']:\n",
    "                temp_anno['feature'] = fea\n",
    "                val_dict['annotations'].append(temp_anno)\n",
    "\n",
    "        val_dict['images'] = val_data['images'].copy()\n",
    "        \n",
    "        json.dump(val_dict, open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/feature/annotations/karpathy_split_' + split + '.json', 'w'))\n",
    "        print(f\"Original image length is {len(val_data['images'])}.  Output length is {len(val_dict['images'])}. {len(val_dict['images'])/len(val_data['images']) * 100}%\")\n",
    "        print(f\"Original annotation length is {val_len}.  Output length is {len(val_dict['annotations'])}. {len(val_dict['annotations'])/val_len * 100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data and join again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_tokenize(text):\n",
    "    # \n",
    "    filters = ['#', '\\$', '@', '&', '!',\n",
    "                '\\t', '\\n', '\\x97', '\\x96', \n",
    "                ',', '\\(', '\\)','\\[','\\]','/',\n",
    "                ';']\n",
    "\n",
    "    # :\n",
    "    # \n",
    "    text = re.sub(\"&quot\", \" \", text, flags=re.S)\n",
    "    text = re.sub(\"=\", \" \", text, flags=re.S)\n",
    "    text = re.sub(\":\", \" \", text, flags=re.S)\n",
    "    text = re.sub(\"\\s:\\s\", \" \", text, flags=re.S)\n",
    "    text = re.sub(\"&amp\", \" \", text, flags=re.S)\n",
    "    text = re.sub(\"AT&amp;T\", \"brand ATT\", text, flags=re.S)\n",
    "    text = re.sub(\"T-Mobile\", \"brand tmobile\", text, flags=re.S)\n",
    "    text = re.sub(\"\\.+\", \" \", text, flags=re.S)\n",
    "    # <>\n",
    "    text = re.sub(\"<.*?>\", \" \", text, flags=re.S)\n",
    "    # n'tnot\n",
    "    text = re.sub(\"n't\", \" not\", text, flags=re.S)\n",
    "    # -\n",
    "    text = re.sub(\"\\s-+\\s\", \" \", text, flags=re.S)\n",
    "    text = re.sub(\"\\s-+$\", \" \", text, flags=re.S)\n",
    "    text = re.sub(\"^-+\\s\", \" \", text, flags=re.S)\n",
    "    # \n",
    "    text = re.sub(\"([0-9]+)%\", \"\\g<1> % \", text, flags=re.S)\n",
    "    # \n",
    "    text = re.sub(\"\\. \", \"  \", text, flags=re.S)\n",
    "    text = re.sub(\"\\.$\", \"  \", text, flags=re.S)\n",
    "    text = re.sub(\"|\".join(filters), \"  \", text, flags=re.S)\n",
    "\n",
    "    # *\n",
    "    text = re.sub(\" \\*([0-9a-zA-Z]+) \", \" \\g<1> \", text, flags=re.S)\n",
    "    text = re.sub(\"\\*([0-9a-zA-Z]+) \", \"\\g<1> \", text, flags=re.S)\n",
    "    text = re.sub(\" \\*([0-9a-zA-Z]+)\", \" \\g<1>\", text, flags=re.S)\n",
    "\n",
    "    text = re.sub(\" ([0-9a-zA-Z]+)\\* \", \" \\g<1> \", text, flags=re.S)\n",
    "    text = re.sub(\"([0-9a-zA-Z]+)\\* \", \"\\g<1> \", text, flags=re.S)\n",
    "    text = re.sub(\" ([0-9a-zA-Z]+)\\*\", \" \\g<1>\", text, flags=re.S)\n",
    "\n",
    "    text = re.sub(\"\\*([0-9a-zA-Z]+)\\* \", \"\\g<1> \", text, flags=re.S)\n",
    "    text = re.sub(\" \\*([0-9a-zA-Z]+)\\* \", \" \\g<1> \", text, flags=re.S)  \n",
    "    text = re.sub(\" \\*([0-9a-zA-Z]+)\\*\", \" \\g<1>\", text, flags=re.S)\n",
    "    \n",
    "    # \n",
    "    text = re.sub(\"(.)\\+(.)\", \"\\g<1> + \\g<2>\", text, flags=re.S)\n",
    "    # -\n",
    "    text = re.sub(\"\\s([a-zA-Z]+)-([a-zA-Z]+)\\s\", \" \\g<1> \\g<2> \", text, flags=re.S)\n",
    "    text = re.sub(\"\\s([a-zA-Z]+)-([a-zA-Z]+)-([a-zA-Z]+)\\s\", \" \\g<1> \\g<2> \\g<3> \", text, flags=re.S)\n",
    "\n",
    "    text = re.sub(\"\\s-+([0-9a-zA-Z]+)\\s\", \" \\g<1> \", text, flags=re.S)\n",
    "    text = re.sub(\"\\s([0-9a-zA-Z]+)-+\\s\", \" \\g<1> \", text, flags=re.S)\n",
    "    text = re.sub(\"\\s([0-9a-zA-Z]+)-+$\", \" \\g<1>\", text, flags=re.S)\n",
    "    text = re.sub(\"\\s-+([0-9a-zA-Z]+)\", \" \\g<1>\", text, flags=re.S)\n",
    "    # text = re.sub(\"-([a-zA-Z]+)\\s\", \"\\g<1> \", text, flags=re.S)\n",
    "    # text = re.sub(\"\\s-([a-zA-Z]+)\", \" \\g<1>\", text, flags=re.S)\n",
    "    # \n",
    "    # text = re.sub(\"|\".join(filters), \" \", text, flags=re.S)\n",
    "    # strip():\n",
    "    result = [i.strip().lower() for i in text.split()]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothing_Shoes_and_Jewelry\n",
      "train\n",
      "Clothing_Shoes_and_Jewelry's train: [5000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [10000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [15000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [20000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [25000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [30000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [35000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [40000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [45000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [50000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [55000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [60000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [65000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [70000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [75000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [80000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [85000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [90000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [95000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [100000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [105000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [110000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [115000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [120000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [125000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [130000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [135000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [140000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [145000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [150000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [155000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [160000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [165000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [170000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [175000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [180000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [185000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [190000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [195000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [200000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [205000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [210000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [215000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [220000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [225000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [230000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [235000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [240000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [245000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [250000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [255000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [260000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [265000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [270000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [275000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [280000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [285000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [290000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [295000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [300000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [305000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [310000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [315000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [320000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [325000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [330000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [335000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [340000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [345000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [350000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [355000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [360000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [365000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [370000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [375000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [380000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [385000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [390000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [395000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [400000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [405000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [410000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [415000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [420000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [425000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [430000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [435000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [440000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [445000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [450000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [455000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [460000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [465000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [470000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [475000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [480000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [485000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [490000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [495000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [500000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [505000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [510000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [515000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [520000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [525000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [530000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [535000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [540000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [545000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [550000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [555000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [560000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [565000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [570000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [575000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [580000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [585000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [590000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [595000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [600000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [605000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [610000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [615000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [620000/626923] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [625000/626923] Tokenized the captions.\n",
      "Original image length is 626923.  Output length is 626923. 100.0%\n",
      "Original annotation length is 626923.  Output length is 626923. 100.0%\n",
      "val\n",
      "Clothing_Shoes_and_Jewelry's val: [5000/14186] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's val: [10000/14186] Tokenized the captions.\n",
      "Original image length is 14186.  Output length is 14186. 100.0%\n",
      "Original annotation length is 14186.  Output length is 14186. 100.0%\n",
      "test\n",
      "Clothing_Shoes_and_Jewelry's test: [5000/14211] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's test: [10000/14211] Tokenized the captions.\n",
      "Original image length is 14211.  Output length is 14211. 100.0%\n",
      "Original annotation length is 14211.  Output length is 14211. 100.0%\n",
      "Home_and_Kitchen\n",
      "train\n",
      "Home_and_Kitchen's train: [5000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [10000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [15000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [20000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [25000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [30000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [35000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [40000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [45000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [50000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [55000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [60000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [65000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [70000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [75000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [80000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [85000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [90000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [95000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [100000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [105000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [110000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [115000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [120000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [125000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [130000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [135000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [140000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [145000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [150000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [155000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [160000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [165000/173150] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [170000/173150] Tokenized the captions.\n",
      "Original image length is 173150.  Output length is 173150. 100.0%\n",
      "Original annotation length is 173150.  Output length is 173150. 100.0%\n",
      "val\n",
      "Home_and_Kitchen's val: [5000/9715] Tokenized the captions.\n",
      "Original image length is 9715.  Output length is 9715. 100.0%\n",
      "Original annotation length is 9715.  Output length is 9715. 100.0%\n",
      "test\n",
      "Home_and_Kitchen's test: [5000/9727] Tokenized the captions.\n",
      "Original image length is 9727.  Output length is 9727. 100.0%\n",
      "Original annotation length is 9727.  Output length is 9727. 100.0%\n",
      "Electronics\n",
      "train\n",
      "Electronics's train: [5000/126635] Tokenized the captions.\n",
      "Electronics's train: [10000/126635] Tokenized the captions.\n",
      "Electronics's train: [15000/126635] Tokenized the captions.\n",
      "Electronics's train: [20000/126635] Tokenized the captions.\n",
      "Electronics's train: [25000/126635] Tokenized the captions.\n",
      "Electronics's train: [30000/126635] Tokenized the captions.\n",
      "Electronics's train: [35000/126635] Tokenized the captions.\n",
      "Electronics's train: [40000/126635] Tokenized the captions.\n",
      "Electronics's train: [45000/126635] Tokenized the captions.\n",
      "Electronics's train: [50000/126635] Tokenized the captions.\n",
      "Electronics's train: [55000/126635] Tokenized the captions.\n",
      "Electronics's train: [60000/126635] Tokenized the captions.\n",
      "Electronics's train: [65000/126635] Tokenized the captions.\n",
      "Electronics's train: [70000/126635] Tokenized the captions.\n",
      "Electronics's train: [75000/126635] Tokenized the captions.\n",
      "Electronics's train: [80000/126635] Tokenized the captions.\n",
      "Electronics's train: [85000/126635] Tokenized the captions.\n",
      "Electronics's train: [90000/126635] Tokenized the captions.\n",
      "Electronics's train: [95000/126635] Tokenized the captions.\n",
      "Electronics's train: [100000/126635] Tokenized the captions.\n",
      "Electronics's train: [105000/126635] Tokenized the captions.\n",
      "Electronics's train: [110000/126635] Tokenized the captions.\n",
      "Electronics's train: [115000/126635] Tokenized the captions.\n",
      "Electronics's train: [120000/126635] Tokenized the captions.\n",
      "Electronics's train: [125000/126635] Tokenized the captions.\n",
      "Original image length is 126635.  Output length is 126635. 100.0%\n",
      "Original annotation length is 126635.  Output length is 126635. 100.0%\n",
      "val\n",
      "Original image length is 4534.  Output length is 4534. 100.0%\n",
      "Original annotation length is 4534.  Output length is 4534. 100.0%\n",
      "test\n",
      "Original image length is 4561.  Output length is 4561. 100.0%\n",
      "Original annotation length is 4561.  Output length is 4561. 100.0%\n",
      "Automotive\n",
      "train\n",
      "Automotive's train: [5000/128212] Tokenized the captions.\n",
      "Automotive's train: [10000/128212] Tokenized the captions.\n",
      "Automotive's train: [15000/128212] Tokenized the captions.\n",
      "Automotive's train: [20000/128212] Tokenized the captions.\n",
      "Automotive's train: [25000/128212] Tokenized the captions.\n",
      "Automotive's train: [30000/128212] Tokenized the captions.\n",
      "Automotive's train: [35000/128212] Tokenized the captions.\n",
      "Automotive's train: [40000/128212] Tokenized the captions.\n",
      "Automotive's train: [45000/128212] Tokenized the captions.\n",
      "Automotive's train: [50000/128212] Tokenized the captions.\n",
      "Automotive's train: [55000/128212] Tokenized the captions.\n",
      "Automotive's train: [60000/128212] Tokenized the captions.\n",
      "Automotive's train: [65000/128212] Tokenized the captions.\n",
      "Automotive's train: [70000/128212] Tokenized the captions.\n",
      "Automotive's train: [75000/128212] Tokenized the captions.\n",
      "Automotive's train: [80000/128212] Tokenized the captions.\n",
      "Automotive's train: [85000/128212] Tokenized the captions.\n",
      "Automotive's train: [90000/128212] Tokenized the captions.\n",
      "Automotive's train: [95000/128212] Tokenized the captions.\n",
      "Automotive's train: [100000/128212] Tokenized the captions.\n",
      "Automotive's train: [105000/128212] Tokenized the captions.\n",
      "Automotive's train: [110000/128212] Tokenized the captions.\n",
      "Automotive's train: [115000/128212] Tokenized the captions.\n",
      "Automotive's train: [120000/128212] Tokenized the captions.\n",
      "Automotive's train: [125000/128212] Tokenized the captions.\n",
      "Original image length is 128212.  Output length is 128212. 100.0%\n",
      "Original annotation length is 128212.  Output length is 128212. 100.0%\n",
      "val\n",
      "Original image length is 4815.  Output length is 4815. 100.0%\n",
      "Original annotation length is 4815.  Output length is 4815. 100.0%\n",
      "test\n",
      "Original image length is 4814.  Output length is 4814. 100.0%\n",
      "Original annotation length is 4814.  Output length is 4814. 100.0%\n",
      "Sports_and_Outdoors\n",
      "train\n",
      "Sports_and_Outdoors's train: [5000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [10000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [15000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [20000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [25000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [30000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [35000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [40000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [45000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [50000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [55000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [60000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [65000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [70000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [75000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [80000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [85000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [90000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [95000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [100000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [105000/110069] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [110000/110069] Tokenized the captions.\n",
      "Original image length is 110069.  Output length is 110069. 100.0%\n",
      "Original annotation length is 110069.  Output length is 110069. 100.0%\n",
      "val\n",
      "Original image length is 4806.  Output length is 4806. 100.0%\n",
      "Original annotation length is 4806.  Output length is 4806. 100.0%\n",
      "test\n",
      "Original image length is 4803.  Output length is 4803. 100.0%\n",
      "Original annotation length is 4803.  Output length is 4803. 100.0%\n"
     ]
    }
   ],
   "source": [
    "for file in new_file_name:\n",
    "    this_file = file[:-4]\n",
    "    print(this_file)\n",
    "    for split in splits:\n",
    "        val_data = json.load(open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/annotations/karpathy_split_' + split + '.json', 'r'))\n",
    "        val_dict = {'type': 'caption', 'images':[], 'annotations': []}\n",
    "        val_len = len(val_data['annotations'])\n",
    "        print(split)\n",
    "        \n",
    "        count = 0\n",
    "        for item in val_data['annotations']:\n",
    "            count+=1\n",
    "            temp_len = 0\n",
    "            temp_anno = {}\n",
    "            temp_anno['image_id'] = item['image_id']\n",
    "            temp_anno['id'] = item['id']\n",
    "\n",
    "            temp_anno['caption'] = ' '.join(title_tokenize(item['caption']))\n",
    "            temp_anno['description'] = ' '.join(title_tokenize(item['description']))\n",
    "            temp_anno['feature'] = item['feature']\n",
    "\n",
    "            val_dict['annotations'].append(temp_anno)\n",
    "            if count % 5000 == 0:\n",
    "                print(f\"{this_file}'s {split}: \" + \"[%d/%d] Tokenized the captions.\" %(count, val_len))\n",
    "\n",
    "        val_dict['images'] = val_data['images'].copy()\n",
    "        \n",
    "        json.dump(val_dict, open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/tokenized/annotations/karpathy_split_' + split + '.json', 'w'))\n",
    "        print(f\"Original image length is {len(val_data['images'])}.  Output length is {len(val_dict['images'])}. {len(val_dict['images'])/len(val_data['images']) * 100}%\")\n",
    "        print(f\"Original annotation length is {val_len}.  Output length is {len(val_dict['annotations'])}. {len(val_dict['annotations'])/val_len * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothing_Shoes_and_Jewelry\n",
      "Clothing_Shoes_and_Jewelry's train: [10000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [20000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [30000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [40000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [50000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [60000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [70000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [80000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [90000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [100000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [110000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [120000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [130000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [140000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [150000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [160000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [170000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [180000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [190000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [200000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [210000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [220000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [230000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [240000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [250000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [260000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [270000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [280000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [290000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [300000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [310000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [320000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [330000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [340000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [350000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [360000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [370000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [380000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [390000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [400000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [410000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [420000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [430000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [440000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [450000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [460000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [470000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [480000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [490000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [500000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [510000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [520000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [530000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [540000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [550000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [560000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [570000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [580000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [590000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [600000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [610000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [620000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [630000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [640000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [650000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [660000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [670000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [680000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [690000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [700000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [710000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [720000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [730000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [740000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [750000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [760000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [770000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [780000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [790000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [800000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [810000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [820000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [830000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [840000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [850000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [860000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [870000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [880000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [890000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [900000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [910000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [920000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [930000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [940000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [950000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [960000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [970000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [980000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [990000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1000000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1010000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1020000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1030000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1040000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1050000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1060000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1070000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1080000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1090000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1100000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1110000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1120000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1130000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1140000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1150000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1160000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1170000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1180000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1190000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1200000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1210000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1220000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1230000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1240000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1250000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1260000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1270000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1280000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1290000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1300000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1310000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1320000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1330000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1340000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1350000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1360000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1370000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1380000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1390000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1400000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1410000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1420000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1430000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1440000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1450000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1460000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1470000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1480000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1490000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1500000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1510000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1520000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1530000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1540000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1550000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1560000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1570000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1580000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1590000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1600000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1610000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1620000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1630000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1640000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1650000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1660000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1670000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1680000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1690000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1700000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1710000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1720000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1730000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1740000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1750000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1760000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1770000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1780000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1790000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1800000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1810000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1820000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1830000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1840000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1850000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1860000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1870000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1880000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1890000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1900000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1910000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1920000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1930000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1940000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1950000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1960000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1970000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1980000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [1990000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2000000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2010000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2020000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2030000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2040000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2050000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2060000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2070000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2080000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2090000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2100000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2110000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2120000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2130000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2140000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2150000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2160000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2170000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2180000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2190000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2200000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2210000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2220000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2230000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2240000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2250000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2260000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2270000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2280000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2290000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2300000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2310000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2320000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2330000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2340000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2350000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2360000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2370000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2380000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2390000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2400000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2410000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2420000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2430000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2440000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2450000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2460000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2470000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2480000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2490000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2500000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2510000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2520000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2530000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2540000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2550000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2560000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2570000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2580000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2590000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2600000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2610000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2620000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2630000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2640000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2650000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2660000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2670000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2680000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2690000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2700000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2710000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2720000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2730000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2740000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2750000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2760000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2770000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2780000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2790000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2800000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2810000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2820000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2830000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2840000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2850000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2860000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2870000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2880000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2890000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2900000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2910000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2920000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2930000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2940000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2950000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2960000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2970000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2980000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [2990000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3000000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3010000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3020000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3030000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3040000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3050000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3060000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3070000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3080000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3090000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3100000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3110000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3120000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3130000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3140000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3150000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3160000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3170000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3180000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3190000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3200000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3210000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3220000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3230000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3240000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3250000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3260000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3270000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3280000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3290000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3300000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3310000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3320000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3330000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3340000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3350000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3360000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3370000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3380000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3390000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3400000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3410000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3420000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3430000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3440000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3450000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3460000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3470000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3480000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3490000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3500000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3510000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3520000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3530000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3540000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3550000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3560000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3570000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3580000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3590000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3600000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3610000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3620000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3630000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3640000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3650000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3660000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3670000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3680000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3690000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3700000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3710000/3725292] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's train: [3720000/3725292] Tokenized the captions.\n",
      "Original image length is 626923.  Output length is 626923. 100.0%\n",
      "Original annotation length is 3725292.  Output length is 3725292. 100.0%\n",
      "Clothing_Shoes_and_Jewelry's val: [10000/84294] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's val: [20000/84294] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's val: [30000/84294] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's val: [40000/84294] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's val: [50000/84294] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's val: [60000/84294] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's val: [70000/84294] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's val: [80000/84294] Tokenized the captions.\n",
      "Original image length is 14186.  Output length is 14186. 100.0%\n",
      "Original annotation length is 84294.  Output length is 84294. 100.0%\n",
      "Clothing_Shoes_and_Jewelry's test: [10000/84771] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's test: [20000/84771] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's test: [30000/84771] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's test: [40000/84771] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's test: [50000/84771] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's test: [60000/84771] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's test: [70000/84771] Tokenized the captions.\n",
      "Clothing_Shoes_and_Jewelry's test: [80000/84771] Tokenized the captions.\n",
      "Original image length is 14211.  Output length is 14211. 100.0%\n",
      "Original annotation length is 84771.  Output length is 84771. 100.0%\n",
      "Home_and_Kitchen\n",
      "Home_and_Kitchen's train: [10000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [20000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [30000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [40000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [50000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [60000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [70000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [80000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [90000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [100000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [110000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [120000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [130000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [140000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [150000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [160000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [170000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [180000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [190000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [200000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [210000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [220000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [230000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [240000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [250000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [260000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [270000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [280000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [290000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [300000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [310000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [320000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [330000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [340000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [350000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [360000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [370000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [380000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [390000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [400000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [410000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [420000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [430000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [440000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [450000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [460000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [470000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [480000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [490000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [500000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [510000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [520000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [530000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [540000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [550000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [560000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [570000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [580000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [590000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [600000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [610000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [620000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [630000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [640000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [650000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [660000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [670000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [680000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [690000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [700000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [710000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [720000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [730000/743763] Tokenized the captions.\n",
      "Home_and_Kitchen's train: [740000/743763] Tokenized the captions.\n",
      "Original image length is 173150.  Output length is 173150. 100.0%\n",
      "Original annotation length is 743763.  Output length is 743763. 100.0%\n",
      "Home_and_Kitchen's val: [10000/41793] Tokenized the captions.\n",
      "Home_and_Kitchen's val: [20000/41793] Tokenized the captions.\n",
      "Home_and_Kitchen's val: [30000/41793] Tokenized the captions.\n",
      "Home_and_Kitchen's val: [40000/41793] Tokenized the captions.\n",
      "Original image length is 9715.  Output length is 9715. 100.0%\n",
      "Original annotation length is 41793.  Output length is 41793. 100.0%\n",
      "Home_and_Kitchen's test: [10000/41615] Tokenized the captions.\n",
      "Home_and_Kitchen's test: [20000/41615] Tokenized the captions.\n",
      "Home_and_Kitchen's test: [30000/41615] Tokenized the captions.\n",
      "Home_and_Kitchen's test: [40000/41615] Tokenized the captions.\n",
      "Original image length is 9727.  Output length is 9727. 100.0%\n",
      "Original annotation length is 41615.  Output length is 41615. 100.0%\n",
      "Electronics\n",
      "Electronics's train: [10000/527575] Tokenized the captions.\n",
      "Electronics's train: [20000/527575] Tokenized the captions.\n",
      "Electronics's train: [30000/527575] Tokenized the captions.\n",
      "Electronics's train: [40000/527575] Tokenized the captions.\n",
      "Electronics's train: [50000/527575] Tokenized the captions.\n",
      "Electronics's train: [60000/527575] Tokenized the captions.\n",
      "Electronics's train: [70000/527575] Tokenized the captions.\n",
      "Electronics's train: [80000/527575] Tokenized the captions.\n",
      "Electronics's train: [90000/527575] Tokenized the captions.\n",
      "Electronics's train: [100000/527575] Tokenized the captions.\n",
      "Electronics's train: [110000/527575] Tokenized the captions.\n",
      "Electronics's train: [120000/527575] Tokenized the captions.\n",
      "Electronics's train: [130000/527575] Tokenized the captions.\n",
      "Electronics's train: [140000/527575] Tokenized the captions.\n",
      "Electronics's train: [150000/527575] Tokenized the captions.\n",
      "Electronics's train: [160000/527575] Tokenized the captions.\n",
      "Electronics's train: [170000/527575] Tokenized the captions.\n",
      "Electronics's train: [180000/527575] Tokenized the captions.\n",
      "Electronics's train: [190000/527575] Tokenized the captions.\n",
      "Electronics's train: [200000/527575] Tokenized the captions.\n",
      "Electronics's train: [210000/527575] Tokenized the captions.\n",
      "Electronics's train: [220000/527575] Tokenized the captions.\n",
      "Electronics's train: [230000/527575] Tokenized the captions.\n",
      "Electronics's train: [240000/527575] Tokenized the captions.\n",
      "Electronics's train: [250000/527575] Tokenized the captions.\n",
      "Electronics's train: [260000/527575] Tokenized the captions.\n",
      "Electronics's train: [270000/527575] Tokenized the captions.\n",
      "Electronics's train: [280000/527575] Tokenized the captions.\n",
      "Electronics's train: [290000/527575] Tokenized the captions.\n",
      "Electronics's train: [300000/527575] Tokenized the captions.\n",
      "Electronics's train: [310000/527575] Tokenized the captions.\n",
      "Electronics's train: [320000/527575] Tokenized the captions.\n",
      "Electronics's train: [330000/527575] Tokenized the captions.\n",
      "Electronics's train: [340000/527575] Tokenized the captions.\n",
      "Electronics's train: [350000/527575] Tokenized the captions.\n",
      "Electronics's train: [360000/527575] Tokenized the captions.\n",
      "Electronics's train: [370000/527575] Tokenized the captions.\n",
      "Electronics's train: [380000/527575] Tokenized the captions.\n",
      "Electronics's train: [390000/527575] Tokenized the captions.\n",
      "Electronics's train: [400000/527575] Tokenized the captions.\n",
      "Electronics's train: [410000/527575] Tokenized the captions.\n",
      "Electronics's train: [420000/527575] Tokenized the captions.\n",
      "Electronics's train: [430000/527575] Tokenized the captions.\n",
      "Electronics's train: [440000/527575] Tokenized the captions.\n",
      "Electronics's train: [450000/527575] Tokenized the captions.\n",
      "Electronics's train: [460000/527575] Tokenized the captions.\n",
      "Electronics's train: [470000/527575] Tokenized the captions.\n",
      "Electronics's train: [480000/527575] Tokenized the captions.\n",
      "Electronics's train: [490000/527575] Tokenized the captions.\n",
      "Electronics's train: [500000/527575] Tokenized the captions.\n",
      "Electronics's train: [510000/527575] Tokenized the captions.\n",
      "Electronics's train: [520000/527575] Tokenized the captions.\n",
      "Original image length is 126635.  Output length is 126635. 100.0%\n",
      "Original annotation length is 527575.  Output length is 527575. 100.0%\n",
      "Electronics's val: [10000/18784] Tokenized the captions.\n",
      "Original image length is 4534.  Output length is 4534. 100.0%\n",
      "Original annotation length is 18784.  Output length is 18784. 100.0%\n",
      "Electronics's test: [10000/18928] Tokenized the captions.\n",
      "Original image length is 4561.  Output length is 4561. 100.0%\n",
      "Original annotation length is 18928.  Output length is 18928. 100.0%\n",
      "Automotive\n",
      "Automotive's train: [10000/472565] Tokenized the captions.\n",
      "Automotive's train: [20000/472565] Tokenized the captions.\n",
      "Automotive's train: [30000/472565] Tokenized the captions.\n",
      "Automotive's train: [40000/472565] Tokenized the captions.\n",
      "Automotive's train: [50000/472565] Tokenized the captions.\n",
      "Automotive's train: [60000/472565] Tokenized the captions.\n",
      "Automotive's train: [70000/472565] Tokenized the captions.\n",
      "Automotive's train: [80000/472565] Tokenized the captions.\n",
      "Automotive's train: [90000/472565] Tokenized the captions.\n",
      "Automotive's train: [100000/472565] Tokenized the captions.\n",
      "Automotive's train: [110000/472565] Tokenized the captions.\n",
      "Automotive's train: [120000/472565] Tokenized the captions.\n",
      "Automotive's train: [130000/472565] Tokenized the captions.\n",
      "Automotive's train: [140000/472565] Tokenized the captions.\n",
      "Automotive's train: [150000/472565] Tokenized the captions.\n",
      "Automotive's train: [160000/472565] Tokenized the captions.\n",
      "Automotive's train: [170000/472565] Tokenized the captions.\n",
      "Automotive's train: [180000/472565] Tokenized the captions.\n",
      "Automotive's train: [190000/472565] Tokenized the captions.\n",
      "Automotive's train: [200000/472565] Tokenized the captions.\n",
      "Automotive's train: [210000/472565] Tokenized the captions.\n",
      "Automotive's train: [220000/472565] Tokenized the captions.\n",
      "Automotive's train: [230000/472565] Tokenized the captions.\n",
      "Automotive's train: [240000/472565] Tokenized the captions.\n",
      "Automotive's train: [250000/472565] Tokenized the captions.\n",
      "Automotive's train: [260000/472565] Tokenized the captions.\n",
      "Automotive's train: [270000/472565] Tokenized the captions.\n",
      "Automotive's train: [280000/472565] Tokenized the captions.\n",
      "Automotive's train: [290000/472565] Tokenized the captions.\n",
      "Automotive's train: [300000/472565] Tokenized the captions.\n",
      "Automotive's train: [310000/472565] Tokenized the captions.\n",
      "Automotive's train: [320000/472565] Tokenized the captions.\n",
      "Automotive's train: [330000/472565] Tokenized the captions.\n",
      "Automotive's train: [340000/472565] Tokenized the captions.\n",
      "Automotive's train: [350000/472565] Tokenized the captions.\n",
      "Automotive's train: [360000/472565] Tokenized the captions.\n",
      "Automotive's train: [370000/472565] Tokenized the captions.\n",
      "Automotive's train: [380000/472565] Tokenized the captions.\n",
      "Automotive's train: [390000/472565] Tokenized the captions.\n",
      "Automotive's train: [400000/472565] Tokenized the captions.\n",
      "Automotive's train: [410000/472565] Tokenized the captions.\n",
      "Automotive's train: [420000/472565] Tokenized the captions.\n",
      "Automotive's train: [430000/472565] Tokenized the captions.\n",
      "Automotive's train: [440000/472565] Tokenized the captions.\n",
      "Automotive's train: [450000/472565] Tokenized the captions.\n",
      "Automotive's train: [460000/472565] Tokenized the captions.\n",
      "Automotive's train: [470000/472565] Tokenized the captions.\n",
      "Original image length is 128212.  Output length is 128212. 100.0%\n",
      "Original annotation length is 472565.  Output length is 472565. 100.0%\n",
      "Automotive's val: [10000/17892] Tokenized the captions.\n",
      "Original image length is 4815.  Output length is 4815. 100.0%\n",
      "Original annotation length is 17892.  Output length is 17892. 100.0%\n",
      "Automotive's test: [10000/17879] Tokenized the captions.\n",
      "Original image length is 4814.  Output length is 4814. 100.0%\n",
      "Original annotation length is 17879.  Output length is 17879. 100.0%\n",
      "Sports_and_Outdoors\n",
      "Sports_and_Outdoors's train: [10000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [20000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [30000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [40000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [50000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [60000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [70000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [80000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [90000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [100000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [110000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [120000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [130000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [140000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [150000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [160000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [170000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [180000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [190000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [200000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [210000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [220000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [230000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [240000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [250000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [260000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [270000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [280000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [290000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [300000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [310000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [320000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [330000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [340000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [350000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [360000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [370000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [380000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [390000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [400000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [410000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [420000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [430000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [440000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [450000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [460000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [470000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [480000/490185] Tokenized the captions.\n",
      "Sports_and_Outdoors's train: [490000/490185] Tokenized the captions.\n",
      "Original image length is 110069.  Output length is 110069. 100.0%\n",
      "Original annotation length is 490185.  Output length is 490185. 100.0%\n",
      "Sports_and_Outdoors's val: [10000/21415] Tokenized the captions.\n",
      "Sports_and_Outdoors's val: [20000/21415] Tokenized the captions.\n",
      "Original image length is 4806.  Output length is 4806. 100.0%\n",
      "Original annotation length is 21415.  Output length is 21415. 100.0%\n",
      "Sports_and_Outdoors's test: [10000/21471] Tokenized the captions.\n",
      "Sports_and_Outdoors's test: [20000/21471] Tokenized the captions.\n",
      "Original image length is 4803.  Output length is 4803. 100.0%\n",
      "Original annotation length is 21471.  Output length is 21471. 100.0%\n"
     ]
    }
   ],
   "source": [
    "for file in new_file_name:\n",
    "    this_file = file[:-4]\n",
    "    print(this_file)\n",
    "    \n",
    "    for split in splits:\n",
    "        val_data = json.load(open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/feature/annotations/karpathy_split_' + split + '.json', 'r'))\n",
    "        val_dict = {'type': 'caption', 'images':[], 'annotations': []}\n",
    "        val_len = len(val_data['annotations'])\n",
    "        \n",
    "        count = 0\n",
    "        for item in val_data['annotations']:\n",
    "            count+=1\n",
    "            temp_len = 0\n",
    "            temp_anno = {}\n",
    "            temp_anno['image_id'] = item['image_id']\n",
    "            temp_anno['id'] = item['id']\n",
    "\n",
    "            temp_anno['caption'] = ' '.join(title_tokenize(item['caption']))\n",
    "            temp_anno['description'] = ' '.join(title_tokenize(item['description']))\n",
    "            temp_anno['feature'] = ' '.join(title_tokenize(item['feature']))\n",
    "\n",
    "            val_dict['annotations'].append(temp_anno)\n",
    "            if count % 10000 == 0:\n",
    "                print(f\"{this_file}'s {split}: \" + \"[%d/%d] Tokenized the captions.\" %(count, val_len))\n",
    "\n",
    "        val_dict['images'] = val_data['images'].copy()\n",
    "        \n",
    "        json.dump(val_dict, open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/tokenized/feature/annotations/karpathy_split_' + split + '.json', 'w'))\n",
    "        print(f\"Original image length is {len(val_data['images'])}.  Output length is {len(val_dict['images'])}. {len(val_dict['images'])/len(val_data['images']) * 100}%\")\n",
    "        print(f\"Original annotation length is {val_len}.  Output length is {len(val_dict['annotations'])}. {len(val_dict['annotations'])/val_len * 100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in new_file_name:\n",
    "    this_file = file[:-4]\n",
    "    print(this_file)\n",
    "    \n",
    "    for split in splits:\n",
    "        val_data = json.load(open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/annotations/karpathy_split_' + split + '.json', 'r'))\n",
    "        total_length = 0\n",
    "        count = 0\n",
    "        for item in val_data['caption']:\n",
    "            temp_length = len(item['caption'].split())\n",
    "            if total_length < temp_length:\n",
    "                total_length = temp_length\n",
    "            if temp_length <= 80:\n",
    "                count += 1\n",
    "        print(f\"{this_file}'s {split} max description length: {total_length}. Less than 80: {count} {count/len(val_data['annotations'])*100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in new_file_name:\n",
    "    this_file = file[:-4]\n",
    "    print(this_file)\n",
    "    \n",
    "    for split in splits:\n",
    "        val_data = json.load(open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/annotations/karpathy_split_' + split + '.json', 'r'))\n",
    "        total_length = 0\n",
    "        count = 0\n",
    "        for item in val_data['annotations']:\n",
    "            temp_length = len(item['description'].split())\n",
    "            if total_length < temp_length:\n",
    "                total_length = temp_length\n",
    "            if temp_length <= 80:\n",
    "                count += 1\n",
    "        print(f\"{this_file}'s {split} max description length: {total_length}. Less than 80: {count} {count/len(val_data['annotations'])*100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename image names and image/file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothing_Shoes_and_Jewelry\n",
      "train\n",
      "save file\n",
      "Original length is 626923.  Output length is 626923. 100.0%\n",
      "val\n",
      "save file\n",
      "Original length is 14186.  Output length is 14186. 100.0%\n",
      "test\n",
      "save file\n",
      "Original length is 14211.  Output length is 14211. 100.0%\n",
      "Home_and_Kitchen\n",
      "train\n",
      "save file\n",
      "Original length is 173150.  Output length is 173150. 100.0%\n",
      "val\n",
      "save file\n",
      "Original length is 9715.  Output length is 9715. 100.0%\n",
      "test\n",
      "save file\n",
      "Original length is 9727.  Output length is 9727. 100.0%\n",
      "Electronics\n",
      "train\n",
      "save file\n",
      "Original length is 126635.  Output length is 126635. 100.0%\n",
      "val\n",
      "save file\n",
      "Original length is 4534.  Output length is 4534. 100.0%\n",
      "test\n",
      "save file\n",
      "Original length is 4561.  Output length is 4561. 100.0%\n",
      "Automotive\n",
      "train\n",
      "save file\n",
      "Original length is 128212.  Output length is 128212. 100.0%\n",
      "val\n",
      "save file\n",
      "Original length is 4815.  Output length is 4815. 100.0%\n",
      "test\n",
      "save file\n",
      "Original length is 4814.  Output length is 4814. 100.0%\n",
      "Sports_and_Outdoors\n",
      "train\n",
      "save file\n",
      "Original length is 110069.  Output length is 110069. 100.0%\n",
      "val\n",
      "save file\n",
      "Original length is 4806.  Output length is 4806. 100.0%\n",
      "test\n",
      "save file\n",
      "Original length is 4803.  Output length is 4803. 100.0%\n"
     ]
    }
   ],
   "source": [
    "for file in new_file_name:\n",
    "    this_file = file[:-4]\n",
    "    print(this_file)\n",
    "    for split in splits:\n",
    "        print(split)\n",
    "        val_data = json.load(open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/tokenized/annotations/karpathy_split_' + split + '.json', 'r'))\n",
    "        val_len = len(val_data['images'])\n",
    "        count = 0\n",
    "        for idx, item in enumerate(val_data['images']):\n",
    "            item['file_name'] = item['file_name'][-14:]\n",
    "        print('save file')\n",
    "        json.dump(val_data, open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/tokenized/annotations/karpathy_split_' + split + '.json', 'w'))\n",
    "        print(f\"Original length is {val_len}.  Output length is {len(val_data['images'])}. {len(val_data['images'])/val_len * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/liufengyuan/NLPinFinance/LFYdata/resized/LFY_2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothing_Shoes_and_Jewelry\n",
      "train\n",
      "val\n",
      "test\n",
      "Home_and_Kitchen\n",
      "train\n",
      "val\n",
      "test\n",
      "Electronics\n",
      "train\n",
      "val\n",
      "test\n",
      "Automotive\n",
      "train\n",
      "val\n",
      "test\n",
      "Sports_and_Outdoors\n",
      "train\n",
      "val\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "my_image_names = []\n",
    "for file in new_file_name:\n",
    "    this_file = file[:-4]\n",
    "    print(this_file)\n",
    "    for split in splits:\n",
    "        print(split)\n",
    "        val_data = json.load(open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/' + this_file + '/tokenized/annotations/karpathy_split_' + split + '.json', 'r'))\n",
    "        for idx, item in enumerate(val_data['images']):\n",
    "            my_image_names.append(item['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4216066"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_img (image):\n",
    "    if image not in my_image_names:\n",
    "        os.remove(path + image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f25e2d0b8b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/liufengyuan/anaconda3/lib/python3.9/logging/__init__.py\", line 227, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for image in image_names:\n",
    "    count+=1\n",
    "    if count % 10000 == 0:\n",
    "        print(\"[%d/%d] Tokenized the captions.\" %(count, len(image_names)))\n",
    "    pool = mp.Pool(48)\n",
    "    pool.apply_async(delete_img, args = (image))\n",
    "print('Close pool')\n",
    "pool.close()\n",
    "print('Join pool')\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28669/3700368517.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevice_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "device_ids = range(  os.cpu_count()/2 )\n",
    "device_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = json.load(open('/data/liufengyuan/NLPinFinance/Five_Categories_Data/Automotive/tokenized/annotations/karpathy_split_val.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': 'B0033YTYKM',\n",
       "  'id': 3058,\n",
       "  'caption': '1987-1993 ford mustang duraflex gt500 front bumper cover 1 piece',\n",
       "  'description': '1987-1993 ford mustang duraflex gt500 front bumper cover 1 piece',\n",
       "  'feature': ['Hand laid, high quality 6 oz. fiberglass',\n",
       "   'Signature black finish',\n",
       "   'Proprietary polymer blend for maximum flexibility',\n",
       "   'High quality weather resistant aluminum mesh grille (where applicable)',\n",
       "   'Reduced damage rate up to 75%']},\n",
       " {'image_id': 'B007KPV8PE',\n",
       "  'id': 3059,\n",
       "  'caption': 'kuryakyn 8851 round shift peg covers for metric bikes',\n",
       "  'description': 'dress up your factory shift peg instead of replacing it this round rubber lined chrome shift peg is now offered in three sizes to accommodate almost any round shift peg',\n",
       "  'feature': ['New - Retail', '1-Year Warranty', 'KURYAKYN KU 8851']},\n",
       " {'image_id': 'B00ANZWJOW',\n",
       "  'id': 3060,\n",
       "  'caption': 'silver tone metal snake head shape gear shift knob grip',\n",
       "  'description': 'perfect car decoration 100 % brand new cobra snake universal manual gear shift knob fit for most vehicles applications can choose red light or blue light four flash modes 1 led light flash 2 led light slow flash 3 led light continuous light 4 led light off specification color silver weight kg 0 16 size 10x7x2 5cm package included 1 x manual gear shift knob 1 x screwdriver 2 x rubber sleeve 8 x screw',\n",
       "  'feature': ['Color:Silver',\n",
       "   'Weight(Kg):0.16',\n",
       "   'Size:10x7x2.5cm',\n",
       "   '100% brand new cobra snake.',\n",
       "   'Fit for most vehicles applications']}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['annotations'][:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c4e795b307436e90878d87b7b6ea0f9ae1f8d88c9c1ffc8ae37ae0851c943d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
